---
title: "Modelos NN"
author: "Jimena Murillo"
date: '2022-05-05'
output:
  word_document: default
  html_document:
    df_print: paged
editor_options:
  chunk_output_type: console
---

#Paquetes

```{r}
library(keras) # for deep learning
library(tidyverse) # general utility functions
library(caret) # machine learning utility functions
library(tibble)
library(readr)
library(ggplot2)
library(tensorflow)
library(lubridate)

```


#Construir una base con el cant贸n de Alajuela y partirla en train y test
```{r}
load("C:/Users/usuario1/Desktop/CIMPA/Github_CIMPA/PRACTICA_CIMPA/base_cantones.RData")
load("base_cantones.RData")



Alajuela1 <- basecanton %>% filter(Canton == "Alajuela")

Alajuela1 <- Alajuela1%>% 
  dplyr::select(Year,Month,Nino12SSTA,Nino3SSTA, Nino4SSTA, Nino34SSTA,TNA,EVI,NDVI,NDWI,LSD,LSN,Precip_t,RR)


str(Alajuela1)

Alajuela1 = Alajuela1 %>% arrange(Year,Month) %>% ungroup() %>% 
  mutate(Month=as.numeric(Month))




if(anyNA(Alajuela1)){
  Alajuela1 <- na.omit(Alajuela1)
}

#Escala


normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}

max <- apply(Alajuela1,2,max)
min <- apply(Alajuela1,2,min)

Alajuela1.2 <- apply(Alajuela1, 2, normalize)

#Train y test

data_train1 = as.data.frame(Alajuela1.2) %>% filter(Year < 0.85) # PARA ENTRENAR HASTA 2018
data_test1 = as.data.frame(Alajuela1.2) %>% filter(Year >= 0.85)

X_train1 = as.matrix(data_train1[,-ncol(data_train1)])
y_train1 = as.matrix(data_train1[,ncol(data_train1)])

X_test1 = as.matrix(data_test1[,-ncol(data_test1)])
y_test1 = as.matrix(data_test1[,ncol(data_test1)])


```



# Datos con lag
```{r}

Alajuela <- basecanton %>% filter(Canton == "Alajuela") %>% 
    dplyr::select(Year,Month,Nino12SSTA, Nino3SSTA, Nino4SSTA,Nino34SSTA,Nino34SSTA1, Nino34SSTA2, Nino34SSTA3, Nino34SSTA4, Nino34SSTA5, Nino34SSTA6, TNA, TNA1,TNA2, EVI, NDVI, NDVI1, NDVI2, NDWI, LSD, LSD1, LSD2, LSN, Precip_t, Precip_t1, Precip_t2, Precip_t3, Precip_t4, Precip_t5, Precip_t6, RRl1, RR) %>% 
  arrange(Year,Month) %>% ungroup() %>% mutate(Month=as.numeric(Month))


if(anyNA(Alajuela)){
  Alajuela <- na.omit(Alajuela)
}

#Escala


normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}

max <- apply(Alajuela,2,max)
min <- apply(Alajuela,2,min)

Alajuela2 <- apply(Alajuela, 2, normalize)


#Train y test

data_train = as.data.frame(Alajuela2) %>% filter(Year < 0.85)#PARA ENTRENAR HASTA 2018
data_test = as.data.frame(Alajuela2) %>% filter(Year >= 0.85)

X_train = as.matrix(data_train[,-ncol(data_train)])
y_train = as.matrix(data_train[,ncol(data_train)])

X_test = as.matrix(data_test[,-ncol(data_test)])
y_test = as.matrix(data_test[,ncol(data_test)])

```

# Modelo inicial simple

```{r}

set.seed(123)
model <- keras_model_sequential()
# our input layer
model %>%
  layer_dense(input_shape = ncol(X_train1), units = 13) %>%
  layer_dense(units = 1, activation = "relu")


# look at our model architecture
summary(model)


model %>% compile(loss = "mse", 
                  optimizer = "adam",
                  metric = "mae")

trained_model <- model %>% fit(
  x = X_train1, # sequence we're using for prediction 
  y = y_train1, # sequence we're predicting
  batch_size = 18, # how many samples to pass to our model at a time
  epochs = 80, # how many times we'll look @ the whole dataset
  validation_split = 0.2) # how much data to hold out for testing as we go along

model %>% evaluate(X_test1, y_test1)

#Escala

denorm <- function(x, max, min) {
  return (x*(max - min)+min)
}

max <- apply(Alajuela1,2,max)
min <- apply(Alajuela1,2,min)

results = model %>% predict(X_test1)
results = denorm(results, max[length(Alajuela1)], min[length(Alajuela1)])

data = cbind(results, Alajuela1[197:235,length(Alajuela1)])
names(data) = c("Resultados", "RR")


Mes = seq(1, length(results))

p <- ggplot(data, aes(x = Mes, y = RR)) + geom_line(colour = "blue")
p <- p +  geom_line(
    aes(x = Mes, y = Resultados),
    colour = "red")

print(p)
    
    ```



#Segundo modelo, se agrega 1 capa

```{r}

set.seed(123)
model2 <- keras_model_sequential()
# our input layer
model2 %>%
  layer_dense(input_shape = ncol(train1), units = 13) %>%
  layer_dense(units = 10, activation = "relu")%>%
  layer_dense(units = 1, activation = "relu")


# look at our model architecture
summary(model2)


model2 %>% compile(loss = "mean_squared_error", 
                  optimizer = "adam",
                  metric = "mean_absolute_error")

trained_model2 <- model2 %>% fit(
  x = X_train1, # sequence we're using for prediction 
  y = y_train1, # sequence we're predicting
  batch_size = 19, # how many samples to pass to our model at a time
  epochs = 60, # how many times we'll look @ the whole dataset
  validation_split = 0.2) # how much data to hold out for testing as we go along

model2 %>% evaluate(X_test1, y_test1)

#Escala

denorm <- function(x, max, min) {
  return (x*(max - min)+min)
}

max <- apply(Alajuela1,2,max)
min <- apply(Alajuela1,2,min)

results = model2 %>% predict(X_test1)
results = denorm(results, max[length(Alajuela1)], min[length(Alajuela1)])

data = cbind(results, Alajuela1[197:nrow(Alajuela1),length(Alajuela1)])
names(data) = c("Resultados", "RR")


Mes = seq(1, length(results))

p <- ggplot(data, aes(x = Mes, y = RR)) + geom_line(colour = "blue") +  
  geom_line( aes(x = Mes, y = Resultados), colour = "red")

print(p)


```

En este modelo se observa una reducci贸n del error cuadrado medio a 0.01.Sin embargo, al graficar se observa una mala predicci贸n

#Modelo con datos con lag

#Preparar datos:


#NN creada con las nuevas variables lag, se ajusta el dropout, y unidades a lo que gener贸 mejores resultados. 


```{r}

set.seed(123)
model3 <- keras_model_sequential()
# our input layer
model3 %>%
  layer_dense(input_shape = ncol(X_train), units = 32) %>%
  layer_dropout(rate = 0.2)%>%
  layer_dense(units = 16, activation = "relu")%>%
  layer_dense(units = 1, activation = "relu")


# look at our model architecture
summary(model3)


model3 %>% compile(loss = "mean_squared_error", 
                  optimizer = "adam",
                  metric = "mean_absolute_error")

trained_model3 <- model3 %>% fit(
  x = X_train, # sequence we're using for prediction 
  y = y_train, # sequence we're predicting
  batch_size = 18, # how many samples to pass to our model at a time
  epochs = 130, # how many times we'll look @ the whole dataset
  validation_split = 0.2) # how much data to hold out for testing as we go along

model3 %>% evaluate(X_test, y_test)

#Escala

denorm <- function(x, max, min) {
  return (x*(max - min)+min)
}

max <- apply(Alajuela,2,max)
min <- apply(Alajuela,2,min)

results = model3 %>% predict(X_test)
results = denorm(results, max[length(Alajuela)], min[length(Alajuela)])

#Escala

data = cbind(results, Alajuela[197:nrow(Alajuela),length(Alajuela)])
colnames(data) = c("Resultados", "RR")
data = as.data.frame(data)

Mes = seq(1, length(results))

p <- ggplot(data, aes(x = Mes, y = RR)) + geom_line(colour = "blue") +
  geom_line(aes(x = Mes, y = Resultados),colour = "red")

print(p)

```

#Se construye un modelo con rnn

```{r}

model5 <- keras_model_sequential()
# our input layer
model5 %>% 
  layer_simple_rnn(units = 24, input_shape = c(ncol(X_train),1), activation='relu') %>% 
  layer_dropout(rate = 0.4)%>%
  layer_dense(units = 12, activation = "relu")%>%
  layer_dense(units = 1, activation = "relu")


# look at our model architecture
summary(model5)


model5 %>% compile(loss = "mean_squared_error", 
                  optimizer = "adam",
                  metric = "mean_absolute_error")

trained_model5 <- model5 %>% fit(
  x = X_train, # sequence we're using for prediction 
  y = y_train, # sequence we're predicting
  batch_size = 18, # how many samples to pass to our model at a time
  epochs = 50, # how many times we'll look @ the whole dataset
  validation_split = 0.2,
  shuffle = F) # how much data to hold out for testing as we go along

model5 %>% evaluate(X_test, y_test)

#Escala

denorm <- function(x, max, min) {
  return (x*(max - min)+min)
}

max <- apply(Alajuela,2,max)
min <- apply(Alajuela,2,min)

results = model5 %>% predict(X_test)

results = denorm(results, max[length(Alajuela)], min[length(Alajuela)])

data = cbind(results, Alajuela[197:nrow(Alajuela),length(Alajuela)])
colnames(data) = c("Resultados", "RR")
data = as.data.frame(data)

Mes = seq(1, length(results))

p <- ggplot(data, aes(x = Mes, y = RR)) + geom_line(colour = "blue") +
  geom_line( aes(x = Mes, y = Resultados), colour = "red")

print(p)


```

# Modelo JP

```{r}

# con nuevas variables
X_train2 <- X_train[,c(2,7,8,9,10,11,12,14,15,16,18,19,20,21,22,23,24,26,27,28,29,30,31,32)]
X_test2 <- X_test[,c(2,7,8,9,10,11,12,14,15,16,18,19,20,21,22,23,24,26,27,28,29,30,31,32)]
colnames(X_train2)

model6 <- keras_model_sequential()
# our input layer
model6 %>% 
  layer_simple_rnn(units = 24, input_shape = c(ncol(X_train2),1), activation='swish') %>% 
  layer_dropout(rate = 0.4)%>%
  layer_dense(units = 12)%>%
  layer_dense(units = 1, activation = "swish")

summary(model6)

model6 %>% compile(loss = "mean_squared_error", 
                  optimizer = "adam",
                  metric = "mean_absolute_error")

trained_model6 <- model6 %>% fit(
  x = X_train2, # sequence we're using for prediction 
  y = y_train, # sequence we're predicting
  batch_size = 18, # how many samples to pass to our model at a time
  epochs = 50, # how many times we'll look @ the whole dataset
  validation_split = 0.2,
  shuffle = F) # how much data to hold out for testing as we go along

model6 %>% evaluate(X_test2, y_test)


#Escala

denorm <- function(x, max, min) {
  return (x*(max - min)+min)
}

max <- apply(Alajuela,2,max)
min <- apply(Alajuela,2,min)

results = model6 %>% predict(X_test2)

results = denorm(results, max[length(Alajuela)], min[length(Alajuela)])

data = cbind(results, Alajuela[197:nrow(Alajuela),length(Alajuela)])
colnames(data) = c("Resultados", "RR")
data = as.data.frame(data)

Mes = seq(1, length(results))

p <- ggplot(data, aes(x = Mes, y = RR)) + geom_line(colour = "blue") +
  geom_line( aes(x = Mes, y = Resultados), colour = "red")

print(p)

```


*formula <- RR~RRl1+Month+P1+P2+P3+P4+P5+P6+N1+N2+N3+N4+N5+N6+T1+T2+L1+L2+NV1+NV2*
    

```{r}
# con nuevas variables
X_train2 <- X_train[,c(2,7,8,9,10,11,12,14,15,16,18,19,20,21,22,23,24,26,27,28,29,30,31,32)]
X_test2 <- X_test[,c(2,7,8,9,10,11,12,14,15,16,18,19,20,21,22,23,24,26,27,28,29,30,31,32)]


model6 <- keras_model_sequential()
# our input layer
model6 %>% 
  layer_simple_rnn(units = 24, input_shape = c(ncol(X_train2),1), activation='relu') %>% 
  layer_dropout(rate = 0.4)%>%
  layer_dense(units = 12)%>%
  layer_dense(units = 1, activation = "relu")

summary(model6)

model6 %>% compile(loss = "mean_squared_error", 
                  optimizer = "adam",
                  metric = "mean_absolute_error")

trained_model6 <- model6 %>% fit(
  x = X_train2, # sequence we're using for prediction 
  y = y_train, # sequence we're predicting
  batch_size = 18, # how many samples to pass to our model at a time
  epochs = 50, # how many times we'll look @ the whole dataset
  validation_split = 0.2,
  shuffle = F) # how much data to hold out for testing as we go along

model6 %>% evaluate(X_test2, y_test)


#Escala

denorm <- function(x, max, min) {
  return (x*(max - min)+min)
}

max <- apply(Alajuela,2,max)
min <- apply(Alajuela,2,min)

results = model6 %>% predict(X_test2)

results = denorm(results, max[length(Alajuela)], min[length(Alajuela)])

data = cbind(results, Alajuela[197:nrow(Alajuela),length(Alajuela)])
colnames(data) = c("Resultados", "RR")
data = as.data.frame(data)

Mes = seq(1, length(results))

p <- ggplot(data, aes(x = Mes, y = RR)) + geom_line(colour = "blue") +
  geom_line( aes(x = Mes, y = Resultados), colour = "red")

print(p)

```


```{r}

# con nuevas variables
X_train2 <- X_train[,c(2,7,8,9,10,11,12,14,15,16,18,19,20,21,22,23,24,26,27,28,29,30,31,32)]
X_test2 <- X_test[,c(2,7,8,9,10,11,12,14,15,16,18,19,20,21,22,23,24,26,27,28,29,30,31,32)]


model6 <- keras_model_sequential()
# our input layer
model6 %>% 
  layer_simple_rnn(units = 24, input_shape = c(ncol(X_train2),1), activation='relu') %>% 
  layer_dropout(rate = 0.8)%>%
  layer_dense(units = 12)%>%
  layer_dropout(rate = 0.2)%>%
  layer_dense(units = 8)%>%
  layer_dense(units = 1, activation = "relu")

summary(model6)

model6 %>% compile(loss = "mean_squared_error", 
                  optimizer = "adam",
                  metric = "mean_absolute_error")

trained_model6 <- model6 %>% fit(
  x = X_train2, # sequence we're using for prediction 
  y = y_train, # sequence we're predicting
  batch_size = 3, # how many samples to pass to our model at a time
  epochs = 25, # how many times we'll look @ the whole dataset
  validation_split = 0.2,
  shuffle = F) # how much data to hold out for testing as we go along

model6 %>% evaluate(X_test2, y_test)



# Escala

denorm <- function(x, max, min) {
  return (x*(max - min)+min)
}

max <- apply(Alajuela,2,max)
min <- apply(Alajuela,2,min)

results = model6 %>% predict(X_test2)
results = denorm(results, max[length(Alajuela)], min[length(Alajuela)])

data = cbind(results, Alajuela[197:nrow(Alajuela),length(Alajuela)])
colnames(data) = c("Resultados", "RR")
data = as.data.frame(data)

Mes = seq(1, length(results))

p <- ggplot(data, aes(x = Mes, y = RR)) + geom_line(colour = "blue") +
  geom_line( aes(x = Mes, y = Resultados), colour = "red")

print(p)

df <- Alajuela %>% select(Year,Month, RR) %>% mutate(fecha=make_date(Year,Month,1))

ggplot(df, aes(x = fecha,y = RR)) + geom_line()

```


# intervalos de confianza con bootstrap

```{r}
#---------------------------------------------------------------------------- Bootstrap ----------------------------------------------------
x <- data$Resultados
n <- 3 # tres meses
Tn <- mean(x) # en este caso lo hago para la media
# muestra bootstrap
B <- 1000
Tboot_b <- NULL
for(b in 1:B) {  
  xb <- sample(x, size = n, replace = TRUE)  
  Tboot_b[b] <- mean(xb)}
# Tboot_b[1:10]

# calculo de estadisticos de bootstrap

Tboot <- mean(Tboot_b)
Vboot <- var(Tboot_b)
sdboot <- sqrt(Vboot)

# sesgo de bootstrap

# mean(Tboot_b) - Tn

# ahora el IC de bootstrap estudentizado
B <- 1000
Tboot_b <- NULL
Tboot_bm <- NULL
sdboot_b <- NULL

for (b in 1:B) {  
  xb <- sample(x, size = n, replace = TRUE)  
  Tboot_b[b] <- mean(xb) # la varianza
  for (m in 1:B) {    
    xbm <- sample(xb, size = n, replace = TRUE)    
    Tboot_bm[m] <- mean(xbm)  
    }  
  sdboot_b[b] <- sd(Tboot_bm)
  }
z_star <- (Tboot_b - Tn) / sdboot_b

# hist(z_star)

# calculo de limites
c(Tn - quantile(z_star, 1 - 0.05 / 2) * sdboot,  Tn - quantile(z_star, 0.05 / 2) * sdboot)
LI = c(data$Resultados - quantile(z_star, 1 - 0.05 / 2) * sdboot)
LS = c(data$Resultados - quantile(z_star, 0.05 / 2) * sdboot)


data$LI = LI
data$LS = LS


p <- ggplot(data, aes(x = Mes, y = RR)) + geom_line(colour = "blue") +
  geom_line( aes(x = Mes, y = Resultados), colour = "red") + 
  geom_ribbon(aes(ymin=LI,ymax=LS), alpha=0.2, fill = "red") +
  xlim(1,3)

print(p)

# ggplot(data, aes(x = Mes, y = RR)) + geom_line(colour = "blue") +
#   geom_line( aes(x = Mes, y = Resultados), colour = "red") + 
#   geom_ribbon(aes(ymin=LI,ymax=LS), alpha=0.2, fill = "red")
```


```{r}
metricas <- function(data){
  NRMSE <- mean((data$Resultados-data$RR)^2)/mean(data$RR)
  NIS_95 <- mean((data$LS-data$LI)+
                   (2/0.05)*(data$LI-data$RR)*(data$RR<data$LI)+
                   (2/0.05)*(data$RR-data$LS)*(data$RR>data$LS))/mean(data$RR)
  return(data.frame(NRMSE,NIS_95))
}

metricas(data)
```

# modelo con mas capas
```{r}

# con nuevas variables
X_train2 <- X_train[,c(2,7,8,9,10,11,12,14,15,16,18,19,20,21,22,23,24,26,27,28,29,30,31,32)]
X_test2 <- X_test[,c(2,7,8,9,10,11,12,14,15,16,18,19,20,21,22,23,24,26,27,28,29,30,31,32)]


model7 <- keras_model_sequential()
# our input layer
model7 %>% 
  layer_simple_rnn(units = 100, input_shape = c(ncol(X_train2),1), activation='swish') %>% 
  layer_dense(units = 50, activation = "relu")%>%
  layer_dense(units = 50, activation = "relu")%>%
  layer_dense(units = 50, activation = "relu")%>%
  layer_dropout(rate = 0.1)%>%
  layer_dense(units = 25, activation = "relu")%>%
  layer_dense(units = 25, activation = "relu")%>%
  layer_dense(units = 25, activation = "relu")%>%
  layer_dropout(rate = 0.1)%>%
  layer_dense(units = 12, activation = "relu")%>%
  layer_dense(units = 12, activation = "relu")%>%
  layer_dropout(rate = 0.1)%>%
  layer_dense(units = 6, activation = "relu")%>%
  layer_dense(units = 6, activation = "relu")%>%
  
  layer_dense(units = 1, activation = "sigmoid")

summary(model7)

model7 %>% compile(loss = "mean_squared_error", 
                  optimizer = "adam",
                  metric = "mean_absolute_error")

trained_model7 <- model7 %>% fit(
  x = X_train2, # sequence we're using for prediction 
  y = y_train, # sequence we're predicting
  batch_size = 3, # how many samples to pass to our model at a time
  epochs = 25, # how many times we'll look @ the whole dataset
  validation_split = 0.2,
  shuffle = F) # how much data to hold out for testing as we go along

model7 %>% evaluate(X_test2, y_test)



# Escala

denorm <- function(x, max, min) {
  return (x*(max - min)+min)
}

max <- apply(Alajuela,2,max)
min <- apply(Alajuela,2,min)

results = model7 %>% predict(X_test2)

results = denorm(results, max[length(Alajuela)], min[length(Alajuela)])

data = cbind(results, Alajuela[197:nrow(Alajuela),length(Alajuela)])
colnames(data) = c("Resultados", "RR")
data = as.data.frame(data)

Mes = seq(1, length(results))

p <- ggplot(data, aes(x = Mes, y = RR)) + geom_line(colour = "blue") +
  geom_line( aes(x = Mes, y = Resultados), colour = "red")

print(p)



```


# intervalos de confianza con bootstrap

```{r}
# bootstrap ----------------------------------------------------
x <- data$Resultados
n <- length(x)
Tn <- mean(x) # en este caso lo hago para la media
# muestra bootstrap
B <- 1000
Tboot_b <- NULL
for(b in 1:B) {  
  xb <- sample(x, size = n, replace = TRUE)  
  Tboot_b[b] <- mean(xb)}
Tboot_b[1:10]

# calculo de estadisticos de bootstrap

(Tboot <- mean(Tboot_b))
(Vboot <- var(Tboot_b))
(sdboot <- sqrt(Vboot))

# sesgo de bootstrap

mean(Tboot_b) - Tn

# ahora el IC de bootstrap estudentizado
B <- 1000
Tboot_b <- NULL
Tboot_bm <- NULL
sdboot_b <- NULL

for (b in 1:B) {  
  xb <- sample(x, size = n, replace = TRUE)  
  Tboot_b[b] <- mean(xb) # la media
  for (m in 1:B) {    
    xbm <- sample(xb, size = n, replace = TRUE)    
    Tboot_bm[m] <- mean(xbm)  
    }  
  sdboot_b[b] <- sd(Tboot_bm)
  }
z_star <- (Tboot_b - Tn) / sdboot_b

hist(z_star)

# calsulo de limites
c(Tn - quantile(z_star, 1 - 0.05 / 2) * sdboot,  Tn - quantile(z_star, 0.05 / 2) * sdboot)
LI = c(data$Resultados - quantile(z_star, 1 - 0.05 / 2) * sdboot)
LS = c(data$Resultados - quantile(z_star, 0.05 / 2) * sdboot)


data$LI = LI
data$LS = LS
head(data)

p <- ggplot(data, aes(x = Mes, y = RR)) + geom_line(colour = "blue") +
  geom_line( aes(x = Mes, y = Resultados), colour = "red") + 
  geom_ribbon(aes(ymin=LI,ymax=LS), alpha=0.2, fill = "red") +
  xlim(1,3) + theme_classic()

print(p)

ggplot(data, aes(x = Mes, y = RR)) + geom_line(colour = "blue") +
  geom_line( aes(x = Mes, y = Resultados), colour = "red") + 
  geom_ribbon(aes(ymin=LI,ymax=LS), alpha=0.2, fill = "red") + theme_classic()
```


```{r}
metricas <- function(data){
  NRMSE <- mean((data$Resultados-data$RR)^2)/mean(data$RR)
  NIS_95 <- mean((data$LS-data$LI)+
                   (2/0.05)*(data$LI-data$RR)*(data$RR<data$LI)+
                   (2/0.05)*(data$RR-data$LS)*(data$RR>data$LS))/mean(data$RR)
  return(data.frame(NRMSE,NIS_95))
}

metricas(data)
```








