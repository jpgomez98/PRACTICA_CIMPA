---
title: "Modelos Cantones JP"
author: "Jose Pablo GÃ³mez Mata"
date: "6/15/2022"
output: html_document
editor_options: 
  chunk_output_type: console
---

# librerias
```{r}
library(tensorflow)
library(keras)
library(tidyverse)
library(caret) 
library(ggplot2)
library(ggpubr)
library(grid)
library(cluster)
library(factoextra)
library(neuralnet)
```
 
# Carga de datos:

```{r}
load("base_cantones.Rdata")

base <- basecanton %>% select(Canton,RR,RRl1, Year,Month,Nino34SSTA1,Nino34SSTA2,Nino34SSTA3,Nino34SSTA4,Nino34SSTA5,Nino34SSTA6,TNA1,TNA2,EVI,NDVI1,NDVI2,NDWI,LSD,LSD1,LSD2,LSN,Precip_t1,Precip_t2,Precip_t3,Precip_t4,Precip_t5,Precip_t6) %>% arrange(Canton,Year,Month) %>% ungroup() %>% mutate(Month=as.numeric(Month))

summary(base)

normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}

base2 <- base %>% group_by(base$Canton) %>% mutate_if(is.numeric,normalize)
summary(base2)

base2 <- base2[,-28]
```


# Entrenamiento y prueba

```{r}
data_train = as.data.frame(base2) %>% filter(Year < 1)# PARA ENTRENAR HASTA 2018
data_test = as.data.frame(base2) %>% filter(Year >= 1)

# data para entrenar
X_train = data_train[,-2] # elimino la respuesta RR
y_train = as.data.frame(data_train[,c(1,2)]) # dejo la respuesta y los cantones

# data para probar
X_test = as.data.frame(data_test[,-2]) # elimino la respuesta RR
y_test = as.data.frame(data_test[,c(1,2)]) # dejo unicamente la respuesta y cantones

```


# Modelo
 
```{r}
# observamos la dmension de entrada 7424 filas (observaciones) x 26 columnas (variables)
dim(X_train)

# creamos el modelo 
mod <- keras_model_sequential()

# capa de entrada
# mod <- layer_input(shape = dim(X_train)[1]) %>% 
#   layer_dense(units = 232, activation = "relu") %>%
#   layer_dense(units = 232, activation = "relu") %>%
#   layer_dense(units = 32, activation = "relu") %>%
#   layer_dense(units = 101, activation = "relu")


mod %>% layer_simple_rnn(units = 500, input_shape = dim(X_train), activation='relu') %>% 
  layer_dense(units = 500, activation = "relu")%>%
  layer_dense(units = 232, activation = "relu")%>%
  layer_dense(units = 101, activation = "relu")%>%
  layer_dense(units = 1, activation = "relu")



summary(mod)





mod %>% evaluate(X_test, y_test)
```




```{r}
Cantones = unique(base$Canton)
Evaluacion3 = matrix(NA, ncol = 2, nrow = length(Cantones))
plot_list3 = list()
plot_list_c = list()
Evaluacion_c = matrix(NA, ncol = 2, nrow = length(Cantones))

for (i in 1:length(Cantones)) {
  
  X_trainc = X_train %>% filter(Canton == Cantones[i])
  X_trainc = as.matrix(X_trainc[,-1])
  y_trainc = y_train %>% filter(Canton == Cantones[i])
  y_trainc = as.matrix(y_trainc[,-1])
  
  X_testc = X_test %>% filter(Canton == Cantones[i])
  X_testc = as.matrix(X_testc[,-1])
  y_testc = y_test %>% filter(Canton == Cantones[i])
  y_testc = as.matrix(y_testc[,-1])
  
  base = as.data.frame(basecanton %>% filter(Canton == Cantones[i]) %>% dplyr::select(RR))
  
  mod %>% compile(optimizer = "adam",
                loss = "mse",
                metrics = "mae")
  
  trained_mod <- mod %>% fit(
  x = X_trainc, # sequence we're using for prediction 
  y =y_trainc, # sequence we're predicting
  batch_size = 25, # how many samples to pass to our model at a time
  epochs = 18, # how many times we'll look @ the whole dataset
  validation_split = 0.2,
  shuffle = F) # how much data to hold out for testing as we go along
}

```
 



